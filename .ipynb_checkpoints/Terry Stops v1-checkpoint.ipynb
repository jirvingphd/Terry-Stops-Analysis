{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seattle Terry Stops Final Project Submission\n",
    "\n",
    "* Student name: Rebecca Mih\n",
    "* Student pace: Part Time Online\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: James Irving\n",
    "* Blog post URL: \n",
    "\n",
    "\n",
    "* Data Source:  https://www.kaggle.com/city-of-seattle/seattle-terry-stops\n",
    "\n",
    "* Date of last update to the datasource: April 15, 2020\n",
    "\n",
    "\n",
    "* Key references:\n",
    "* https://assets.documentcloud.org/documents/6136893/SPDs-2019-Annual-Report-on-Stops-and-Detentions.pdf\n",
    "\n",
    "* https://www.seattletimes.com/seattle-news/crime/federal-monitor-finds-seattle-police-are-conducting-proper-stops-and-frisks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src= \"Terry Stops Sagepub.com.png\"\n",
    "           width=500\"/>\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "https://caselaw.findlaw.com/us-supreme-court/392/1.html\n",
    "\n",
    "This data represents records of police reported stops under Terry v. Ohio, 392 U.S. 1 (1968). Each row represents a unique stop.\n",
    "\n",
    " A Terry stop is a seizure under both state and federal law. A Terry stop is\n",
    "defined in policy as a brief, minimally intrusive seizure of a subject based upon\n",
    "**articulable reasonable suspicion (ARS) in order to investigate possible criminal activity.**\n",
    "The stop can apply to people as well as to vehicles. The subject of a Terry stop is\n",
    "**not** free to leave.\n",
    "\n",
    "Section 6.220 of the Seattle Police Department (SPD) Manual defines Reasonable Suspicion as:\n",
    "Specific, objective, articulable facts which, taken together with rational inferences, would\n",
    "create a  **well-founded suspicion that there is a substantial possibility that a subject has\n",
    "engaged, is engaging or is about to engage in criminal conduct.**\n",
    "\n",
    "- Each record contains perceived demographics of the subject, as reported by the officer making the stop and officer demographics as reported to the Seattle Police Department, for employment purposes.\n",
    "- Where available, data elements from the associated Computer Aided Dispatch (CAD) event (e.g. Call Type, Initial Call Type, Final Call Type) are included.\n",
    "\n",
    "\n",
    "## Notes on Concealed Weapons in the State of Washington\n",
    "\n",
    "WHAT ARE WASHINGTON’S CONCEALED CARRY LAWS?\n",
    "Open carry of a firearm is lawful without a permit in the state of Washington except, according to the law, “under circumstances, and at a time and place that either manifests an intent to intimidate another or that warrants alarm for the safety of other persons.”\n",
    "\n",
    "However, open carry of a loaded handgun in a vehicle is legal only with a concealed pistol license. Open carry of a loaded long gun in a vehicle is illegal.\n",
    "\n",
    "The criminal charge of “carrying a concealed firearm” happens in this state when someone carries a concealed firearm without a concealed pistol license. It does not matter if the weapon was discovered in the defendant’s home, vehicle, or on his or her person.\n",
    "\n",
    "## Objectives\n",
    "### Target:\n",
    "\n",
    "   * Identify Terry Stops which lead to Arrest or Prosecution (Binary Classification)\n",
    "    \n",
    "### Features:\n",
    "   * Location (Precinct)\n",
    "   * Day of the Week (Date)\n",
    "   * Shift (Time)\n",
    "   * Initial Call Type\n",
    "   * Final Call Type\n",
    "   * Stop Resolution\n",
    "   * Weapon type\n",
    "   * Officer Squad\n",
    "   * Age of officer\n",
    "   * Age of detainee\n",
    "    \n",
    "    \n",
    "### Optional Features:\n",
    "   * Race of officer\n",
    "   * Race of detainee\n",
    "   * Gender of officer\n",
    "   * Gender of detainee\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of features provided\n",
    "#\n",
    "Column Names and descriptions\n",
    "\n",
    "* **Subject Age Group**\t\n",
    "Subject Age Group (10 year increments) as reported by the officer.  **Label encoding**  <br><br>\n",
    "\n",
    "* **Subject ID**\t\n",
    "Key, generated daily, identifying unique subjects in the dataset using a character to character match of first name and last name. \"Null\" values indicate an \"anonymous\" or \"unidentified\" subject. Subjects of a Terry Stop are not required to present identification.  **Not Used** <br><br>\n",
    "\n",
    "* **GO / SC Num**\n",
    "General Offense or Street Check number, relating the Terry Stop to the parent report. This field may have a one to many relationship in the data. **Not Used** <br><br>\n",
    "\n",
    "* **Terry Stop ID**\n",
    "Key identifying unique Terry Stop reports.  **Not Used**\n",
    "<br><br>\n",
    "\n",
    "\n",
    "* **Stop Resolution**\n",
    "Resolution of the stop as reported by the officer. \n",
    "**One hot encoding**\n",
    "\n",
    "\n",
    "* **Weapon Type**\t\n",
    "Type of weapon, if any, identified during a search or frisk of the subject. Indicates \"None\" if no weapons was found. **One hot encoding**\n",
    "\n",
    "* **Officer ID**\t\n",
    "Key identifying unique officers in the dataset.\n",
    "**Not Used**\n",
    "\n",
    "\n",
    "* **Officer YOB**\t\n",
    "Year of birth, as reported by the officer.\n",
    "\n",
    "\n",
    "* **Officer Gender**\t\n",
    "Gender of the officer, as reported by the officer.\n",
    "**One Hot Encoding**\n",
    "\n",
    "\n",
    "* **Officer Race**\t\n",
    "Race of the officer, as reported by the officer.\n",
    "\n",
    "\n",
    "* **Subject Perceived Race**\t\n",
    "Perceived race of the subject, as reported by the officer.\n",
    "\n",
    "\n",
    "* **Subject Perceived Gender**\t\n",
    "Perceived gender of the subject, as reported by the officer.\n",
    "\n",
    "\n",
    "* **Reported Date**\t\n",
    "Date the report was filed in the Records Management System (RMS). Not necessarily the date the stop occurred but generally within 1 day.\n",
    "\n",
    "\n",
    "* **Reported Time**\t\n",
    "Time the stop was reported in the Records Management System (RMS). Not the time the stop occurred but generally within 10 hours.\n",
    "\n",
    "\n",
    "* **Initial Call Type**\t\n",
    "Initial classification of the call as assigned by 911.\n",
    "\n",
    "\n",
    "* **Final Call Type**\t\n",
    "Final classification of the call as assigned by the primary officer closing the event.\n",
    "\n",
    "\n",
    "* **Call Type**\t\n",
    "How the call was received by the communication center.\n",
    "\n",
    "\n",
    "* **Officer Squad**\t\n",
    "Functional squad assignment (not budget) of the officer as reported by the Data Analytics Platform (DAP).\n",
    "\n",
    "\n",
    "* **Arrest Flag**\t\n",
    "Indicator of whether a \"physical arrest\" was made, of the subject, during the Terry Stop. Does not necessarily reflect a report of an arrest in the Records Management System (RMS).\n",
    "\n",
    "\n",
    "* **Frisk Flag**\t\n",
    "Indicator of whether a \"frisk\" was conducted, by the officer, of the subject, during the Terry Stop. **Not Used**\n",
    "\n",
    "\n",
    "* **Precinct**\t\n",
    "Precinct of the address associated with the underlying Computer Aided Dispatch (CAD) event. Not necessarily where the Terry Stop occurred.\n",
    "\n",
    "\n",
    "* **Sector**\t\n",
    "Sector of the address associated with the underlying Computer Aided Dispatch (CAD) event. Not necessarily where the Terry Stop occurred. \n",
    "**Not Used**\n",
    "\n",
    "* **Beat**\t\n",
    "Beat of the address associated with the underlying Computer Aided Dispatch (CAD) event. Not necessarily where the Terry Stop occurred.\n",
    "**Not Used**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Workflow (OSEMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Obtain and Pre-process**\n",
    "    - [x] Import data\n",
    "    - [x] Remove unused columns\n",
    "    - [x] Check data size, NaNs, and # of non-null values which are not valid data \n",
    "    - [x] Clean up missing values by imputing values or dropping\n",
    "    - [x] Replace ? or other non-valid data by imputing values or dropping data\n",
    "    - [x] Check for duplicates and remove if appropriate\n",
    "    - [x] Change datatypes of columns as appropriate \n",
    "    - [x] Note which features are continuous and which are categorical<br><br>\n",
    "\n",
    "2. **Perform visualizations to get a sense of the data**\n",
    "     - [x] Use .describe() and .hist() histograms\n",
    "     - [x] Identify outliers (based on auto-scaling of plots) and remove or inpute as needed\n",
    "     - [x] Perform visualizations on key features to understand  \n",
    "     - [x] Look at feature correlations (Pearson correlation) for those that may be co-linear**<br><br>\n",
    "   \n",
    "3. **Transformation of data**\n",
    "    - [x] Use re-sampling if needed to balance the dataset\n",
    "    - [x] Transform categoricals using 1-hot encoding or label encoding/\n",
    "    - [x] Perform log transformations on continuous variables (if applicable)\n",
    "    - [x] Normalize continuous variables <br> <br>\n",
    "\n",
    "4.  **Create a Vanilla Machine Learning Model**\n",
    "    - [x] Split into train and test data \n",
    "    - [x] Run the model\n",
    "    - [x] Review Quality indicators of the model <br><br>\n",
    "    \n",
    "5. **Run more advanced models**\n",
    "    - [x] Compare the model quality\n",
    "    - [x] Choose one or more models for grid searching <br><br>\n",
    "    \n",
    "6. **Revise data inputs if needed to improve quality indicators**\n",
    "    - [x] By adding created features, and removing colinear features\n",
    "    - [x] by removing outliers through filters\n",
    "    - [x] through use of subject matter knowledge <br><br>\n",
    "    \n",
    "7. **Write the Report**\n",
    "    - [X] Explain key findings and recommended next steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain and Pre-Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Obtain and Pre-process**\n",
    "    - [x] Import data\n",
    "    - [x] Remove unused columns\n",
    "    - [x] Check data size, NaNs, and # of non-null values which are not valid data \n",
    "    - [x] Clean up missing values by imputing values or dropping\n",
    "    - [x] Replace ? or other non-valid data by imputing values or dropping data\n",
    "    - [x] Check for duplicates and remove if appropriate\n",
    "    - [x] Change datatypes of columns as appropriate \n",
    "    - [x] Note which features are continuous and which are categorical<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import sklearn\n",
    "import math\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Terry_Stops.csv',low_memory=False)\n",
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Age Group</th>\n",
       "      <th>Subject ID</th>\n",
       "      <th>GO / SC Num</th>\n",
       "      <th>Terry Stop ID</th>\n",
       "      <th>Stop Resolution</th>\n",
       "      <th>Weapon Type</th>\n",
       "      <th>Officer ID</th>\n",
       "      <th>Officer YOB</th>\n",
       "      <th>Officer Gender</th>\n",
       "      <th>Officer Race</th>\n",
       "      <th>...</th>\n",
       "      <th>Reported Time</th>\n",
       "      <th>Initial Call Type</th>\n",
       "      <th>Final Call Type</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>Officer Squad</th>\n",
       "      <th>Arrest Flag</th>\n",
       "      <th>Frisk Flag</th>\n",
       "      <th>Precinct</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Beat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>20140000120677</td>\n",
       "      <td>92317</td>\n",
       "      <td>Arrest</td>\n",
       "      <td>None</td>\n",
       "      <td>7500</td>\n",
       "      <td>1984</td>\n",
       "      <td>M</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>...</td>\n",
       "      <td>11:32:00</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>SOUTH PCT 1ST W - ROBERT</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>South</td>\n",
       "      <td>O</td>\n",
       "      <td>O2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>20150000001670</td>\n",
       "      <td>32260</td>\n",
       "      <td>Field Contact</td>\n",
       "      <td>None</td>\n",
       "      <td>7539</td>\n",
       "      <td>1963</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>04:55:00</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>20150000002451</td>\n",
       "      <td>46430</td>\n",
       "      <td>Field Contact</td>\n",
       "      <td>None</td>\n",
       "      <td>7591</td>\n",
       "      <td>1985</td>\n",
       "      <td>M</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>...</td>\n",
       "      <td>01:06:00</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>WEST PCT 3RD W - MARY</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>20150000002815</td>\n",
       "      <td>51725</td>\n",
       "      <td>Field Contact</td>\n",
       "      <td>None</td>\n",
       "      <td>7456</td>\n",
       "      <td>1979</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>19:27:00</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NORTH PCT 2ND W - NORA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>20150000002815</td>\n",
       "      <td>51727</td>\n",
       "      <td>Field Contact</td>\n",
       "      <td>None</td>\n",
       "      <td>7456</td>\n",
       "      <td>1979</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>19:32:00</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NORTH PCT 2ND W - NORA</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject Age Group  Subject ID     GO / SC Num  Terry Stop ID  \\\n",
       "0                 -          -1  20140000120677          92317   \n",
       "1                 -          -1  20150000001670          32260   \n",
       "2                 -          -1  20150000002451          46430   \n",
       "3                 -          -1  20150000002815          51725   \n",
       "4                 -          -1  20150000002815          51727   \n",
       "\n",
       "  Stop Resolution Weapon Type Officer ID  Officer YOB Officer Gender  \\\n",
       "0          Arrest        None     7500           1984              M   \n",
       "1   Field Contact        None     7539           1963              M   \n",
       "2   Field Contact        None     7591           1985              M   \n",
       "3   Field Contact        None     7456           1979              M   \n",
       "4   Field Contact        None     7456           1979              M   \n",
       "\n",
       "                Officer Race   ...   Reported Time Initial Call Type  \\\n",
       "0  Black or African American   ...        11:32:00                 -   \n",
       "1                      White   ...        04:55:00                 -   \n",
       "2         Hispanic or Latino   ...        01:06:00                 -   \n",
       "3                      White   ...        19:27:00                 -   \n",
       "4                      White   ...        19:32:00                 -   \n",
       "\n",
       "  Final Call Type Call Type             Officer Squad Arrest Flag Frisk Flag  \\\n",
       "0               -         -  SOUTH PCT 1ST W - ROBERT           N          N   \n",
       "1               -         -                       NaN           N          N   \n",
       "2               -         -     WEST PCT 3RD W - MARY           N          N   \n",
       "3               -         -    NORTH PCT 2ND W - NORA           N          N   \n",
       "4               -         -    NORTH PCT 2ND W - NORA           N          N   \n",
       "\n",
       "  Precinct  Sector    Beat  \n",
       "0    South  O       O2      \n",
       "1        -       -       -  \n",
       "2        -       -       -  \n",
       "3        -       -       -  \n",
       "4        -       -       -  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop Columns which contain IDs, which are not useful features,  Frisk flag (we are only looking at arrests), and Sector and Beat which are more granular location information (we will keep precinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Subject ID', 'GO / SC Num', 'Terry Stop ID', 'Officer ID', \n",
    "                  \"Call Type\", 'Frisk Flag', 'Sector', 'Beat'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()\n",
    "# After dropping some of the columns, some rows appear to be duplicated.\n",
    "# However, since the date and time of the incident are NOT exact (i.e. the date could be 24 hours later, and the\n",
    "# time could be 10 hours later), it's possible to get some that are similar on different consecutive dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Subject Age Group', 'Stop Resolution', 'Weapon Type', 'Officer YOB',\n",
       "       'Officer Gender', 'Officer Race', 'Subject Perceived Race',\n",
       "       'Subject Perceived Gender', 'Reported Date', 'Reported Time',\n",
       "       'Initial Call Type', 'Final Call Type', 'Officer Squad', 'Arrest Flag',\n",
       "       'Precinct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Subject Age Group', 'Stop Resolution', 'Weapon Type', 'Officer YOB',\n",
      "       'Officer Gender', 'Officer Race', 'Subject Perceived Race',\n",
      "       'Subject Perceived Gender', 'Reported Date', 'Reported Time',\n",
      "       'Initial Call Type', 'Final Call Type', 'Officer Squad', 'Arrest Flag',\n",
      "       'Precinct'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "col_names = df.columns\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41104, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "\n",
    "# The rationale for this is to understand how big the dataset is,  how many features are contained in the data\n",
    "# This helps with planning for function vs lambda functions,  and whether certain kinds of visualizations will be feasible\n",
    "# for the analysis (with my computer hardware).  With compute limitations, types of correlation plots cause the kernal to die,\n",
    "# if there are more than 11 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "* df.isna().sum()\n",
    "\n",
    "isna().sum() determines how many data are missing from a given feature\n",
    "\n",
    "* df.info() \n",
    "\n",
    "df.info() helps you determine if there missing values or datatypes that need to be modified\n",
    "\n",
    "* Handy alternate checks if needed **\n",
    "    - [x] df.isna().any()\n",
    "    - [x] df.isnull().any()\n",
    "    - [x] df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject Age Group             0\n",
       "Stop Resolution               0\n",
       "Weapon Type                   0\n",
       "Officer YOB                   0\n",
       "Officer Gender                0\n",
       "Officer Race                  0\n",
       "Subject Perceived Race        0\n",
       "Subject Perceived Gender      0\n",
       "Reported Date                 0\n",
       "Reported Time                 0\n",
       "Initial Call Type             0\n",
       "Final Call Type               0\n",
       "Officer Squad               535\n",
       "Arrest Flag                   0\n",
       "Precinct                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Officer Squad'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Findings from isna().sum() *\n",
    "* Officer Squad has 535 missing data (1.3% of the data)\n",
    "    * Impute \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subject Age Group           0\n",
       "Stop Resolution             0\n",
       "Weapon Type                 0\n",
       "Officer YOB                 0\n",
       "Officer Gender              0\n",
       "Officer Race                0\n",
       "Subject Perceived Race      0\n",
       "Subject Perceived Gender    0\n",
       "Reported Date               0\n",
       "Reported Time               0\n",
       "Initial Call Type           0\n",
       "Final Call Type             0\n",
       "Officer Squad               0\n",
       "Arrest Flag                 0\n",
       "Precinct                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41104 entries, 0 to 41103\n",
      "Data columns (total 15 columns):\n",
      "Subject Age Group           41104 non-null object\n",
      "Stop Resolution             41104 non-null object\n",
      "Weapon Type                 41104 non-null object\n",
      "Officer YOB                 41104 non-null int64\n",
      "Officer Gender              41104 non-null object\n",
      "Officer Race                41104 non-null object\n",
      "Subject Perceived Race      41104 non-null object\n",
      "Subject Perceived Gender    41104 non-null object\n",
      "Reported Date               41104 non-null object\n",
      "Reported Time               41104 non-null object\n",
      "Initial Call Type           41104 non-null object\n",
      "Final Call Type             41104 non-null object\n",
      "Officer Squad               41104 non-null object\n",
      "Arrest Flag                 41104 non-null object\n",
      "Precinct                    41104 non-null object\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(keep = False)]\n",
    "duplicates.head(118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Look for extraneous values using value_counts()\n",
    "1. Rationale:  This analysis is useful for flushing out missing values in the form\n",
    "of question marks, or dummy variables\n",
    "2.  It also gives a preliminary view of the distributions in the data, albeit by numbers rather than graphics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, '\\n', df[col].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Findings from value_counts():\n",
    "\n",
    "## value_counts()  is a way to discover non-null invalid data and non-unique data\n",
    "\n",
    "1. The \"-\" is used as a substitute for unknown, in many cases.  Perhaps it would be good to build a function to impute \"unknown\" for the \"-\" for multiple features\n",
    "2. Race and gender need re-mapping\n",
    "3. Call Types, Weapons, Officer Squads and Precincts need re-mapping\n",
    "4. Date and time need to be changed to day of the week, and into police watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how many non-unique ids there are in the data\n",
    "len(df.id) - seattle_df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "#duplicates = seattle_df[seattle_df.duplicated(subset =['id'], keep = False)]\n",
    "#duplicates.sort_values(by=['id']).head()\n",
    "duplicates = df[df.duplicated(keep = False)]\n",
    "duplicates.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finding from duplicated():\n",
    "- If you look at the beginning of the analysis, I checked for duplications with the entire dataset (before removing columns of data, such as \"ID\"),  there were no duplicates. But after dropping the ID,  there are 118 rows in duplication, 109 pairs. \n",
    "\n",
    "- Because the date and time are not exact (the documentation says sometimes the date could have been entered 24 hours later, or the time could be off by 10 hours, so that actually unique Terry stops could have the same data (when the ID columns are removed).\n",
    "\n",
    "- There are a few that are arrests.  Still open to decide whether to remove the duplicated data or not.  \n",
    "\n",
    "- What is curious is that the index number is not always consecutive between different pairs of duplicates.  This suggests that perhaps the data was input twice -- maybe due to some computer or internet glitches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps - Clean up:\n",
    "\n",
    "    * 1. Update the Missing data in waterfront, view, yr_renovated\n",
    "    * 2. ? in sqft_basement\n",
    "    * 3. Convert sqft_basement to float or int\n",
    "    * 4. Update / impute values for bathrooms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not shown analysis:  I imputed dummy 1.5 into a test df, then test.groupby(['view'])['price'].describe()\n",
    "# In all 3 cases,  the dummy \"1.5\" distribution looks like the \"0.0\"  distribution,  so go ahead and group together\n",
    "\n",
    "seattle_df.waterfront.fillna(value = 0.0, inplace = True)\n",
    "seattle_df.view.fillna(value = 0.0, inplace = True)\n",
    "seattle_df.yr_renovated.fillna(value = 0.0, inplace = True)\n",
    "\n",
    "# Confirm that the missing values have been replaced\n",
    "seattle_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the 'sqft_basement' column of data\n",
    "\n",
    "1. Convert the 'sqft_basement' columns data to strings \n",
    "2. Use string methods to remove the '?' and the '.0' endings\n",
    "3. Then convert the data into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df.to_string(columns = ['sqft_basement'])\n",
    "seattle_df.sqft_basement.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas String Replacement to remove the \".0\" from each value in order to make these integers\n",
    "# Reference https://www.geeksforgeeks.org/python-pandas-series-str-slice/\n",
    "#seattle_df['sqft_basement'] = seattle_df['sqft_basement'].astype(str)\n",
    "seattle_df.sqft_basement.replace(to_replace = '?', value = 0.0, inplace = True)\n",
    "seattle_df.sqft_basement.replace(to_replace = '0.0', value = 0.0, inplace = True)\n",
    "seattle_df.sqft_basement = seattle_df.sqft_basement.astype(str)\n",
    "seattle_df.sqft_basement.value_counts().head()\n",
    "start, stop, step = 0, -2, 1\n",
    "seattle_df['sqft_basement'] = seattle_df['sqft_basement'].str.slice(start, stop, step)\n",
    "seattle_df.sqft_basement.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now change all the values in sqft_basement into integers\n",
    "seattle_df.sqft_basement = seattle_df.sqft_basement.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check to see if there are discrepancies between the sqft living and the sum of sqft_above + sqft_basement\n",
    "seattle_df['sqft_check'] = seattle_df['sqft_above'] + seattle_df['sqft_basement']\n",
    "\n",
    "seattle_df['sqft_check'].dtypes\n",
    "print((seattle_df['sqft_check'] == seattle_df['sqft_living']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if there are any errors in calculating sqft_living from sqft_above + sqft_basement\n",
    "bigger = 0\n",
    "smaller= 0\n",
    "\n",
    "for footage in range(0, len(seattle_df)):\n",
    "    if seattle_df['sqft_check'][footage] != seattle_df['sqft_living'][footage]:        \n",
    "        if seattle_df.sqft_living[footage] > seattle_df.sqft_check[footage]:\n",
    "                bigger += 1\n",
    "        else:\n",
    "                smaller +=1\n",
    "                \n",
    "print(\" Sqft_living was bigger than calculated value {} number of times\".format(bigger))\n",
    "print(\" Sqft_living was smaller than calculated value {} number of times\".format(smaller))\n",
    "\n",
    "#  Question:  Is there a way to do this without creating another column in the df that I then have to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df.drop(columns = ['sqft_check', 'id', 'date'], inplace=True)\n",
    "#seattle_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:  Sqft_living is always bigger than sqft_above + sqft_basement\n",
    "\n",
    "    * Recommendation:  use Sqft_living and sqft_basement as features to be fed to the model only,  since sqft_above is an extra value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column (feature) for price per square foot. \n",
    "seattle_df['ppsf'] = seattle_df['price'] / seattle_df['sqft_living']\n",
    "\n",
    "#seattle_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at other features for outliers using scatter plots versus price\n",
    "\n",
    "* We've cleaned up the missing data and ? kinds of data.\n",
    "* However,  there may still be typos or other messy data which are outliers, which should be cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(16,3))\n",
    "for xcol, ax in zip(list(seattle_df)[1:7], axes):\n",
    "                    seattle_df.plot(kind='scatter', x=xcol, y='price', ax=ax, alpha = 0.4, color = 'b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(16,3))\n",
    "for xcol, ax in zip(list(seattle_df)[8:14], axes):\n",
    "                    seattle_df.plot(kind='scatter', x=xcol, y='price', ax=ax, alpha = 0.4, color = 'b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(16,3))\n",
    "for xcol, ax in zip(list(seattle_df)[15:22], axes):\n",
    "                    seattle_df.plot(kind='scatter', x=xcol, y='price', ax=ax, alpha = 0.4, color = 'b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoscaling allows to see clearly there is an outlier in one house that has 33 bedroms.\n",
    "\n",
    "### Bathrooms looks continuous when it probabaly ought be a categorical.  There were odd numbers of bathrooms  (1.2, 1.8) seen in the data\n",
    "\n",
    "### There may be outliers in sqft_living,  sqft_lot,  sqft_lot15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seattle_df.sort_values(by=['bedrooms'], ascending = False).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like the house with 33 bedrooms, is probably an outlier as the bedrooms\n",
    "# inconsistent with the sqft_living and bathrooms and price\n",
    "# Assume that the \"33\"  is a typo and should have been \"3\" Bedrooms - impute 3 for 33\n",
    "seattle_df['bedrooms'].replace(to_replace = 33, value = 3, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df.groupby('bathrooms')['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems that it's potentially ok\n",
    "# King County may count \"quarter baths\" as well as \"half baths\", which could account for the unusual bath numbers\n",
    "# Based on the distribution,  probably the number of baths should be treated as a categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seattle_df.sqft_lot.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot price and price per square foot distributions\n",
    "# There are some huge lots -- these probably should be filtered\n",
    "# Visualization to Determine Ways to Condition the Data\n",
    "# References: \n",
    "# https://jovianlin.io/data-visualization-seaborn-part-2/\n",
    "# https://jovianlin.io/data-visualization-seaborn-part-3/\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "title = fig.suptitle(\"Home and Lot Size Distribution in King County\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.35, )\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_title(\"Living Area Distribution\", fontsize=12)\n",
    "ax1.set_ylabel(\"Distribution A.U.\",)\n",
    "#ax1.tick_params(labelsize=12)\n",
    "sns.distplot(seattle_df['sqft_living'], ax=ax1, color='b', axlabel = 'Living Area (Sq Ft)');\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_title(\"Lot Area Distribution\",)\n",
    "ax2.set_ylabel(\"Distribution A.U.\") \n",
    "sns.distplot(seattle_df['sqft_lot'], ax=ax2, color='r', axlabel = \"Lot Area (Sq Ft)\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df['sqft_lot'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One can see there outliers in the lot sizes, which could be cleaned up\n",
    "\n",
    "## We can remove outliers in data through looking at the statistics with a boxplot.  \n",
    "\n",
    "## What's a boxplot?\n",
    "\n",
    "<div>\n",
    "<img src=\"Boxplot.gif\" width=\"500\"/>\n",
    "</div\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51\n",
    "\n",
    "\n",
    "* The method of detection of outliers (and subsequent removal) assumes the data is normally distributed. Because of the asymmetry of the distribution, only those on the right side of the distibution are likely to be removed\n",
    "* One question is what is the most appropriate way to handle outliers, we will try interquartile means below\n",
    "\n",
    "\n",
    "* Reference :  https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623\n",
    "* Reference: https://machinelearningmastery.com/how-to-calculate-the-5-number-summary-for-your-data-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(df, col):\n",
    "    \"Function to calculated outliers based on a normal distribution and remove those data from the dataframe\"\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    df_max, df_min = df[col].max(), df[col].min()\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_below = Q1 - 1.5 * IQR\n",
    "    outliers_above = Q3 + 1.5 * IQR\n",
    "\n",
    "    print(\"{} metrics: Min ${:.0f},  Low Outlier ${:.0f},  Q1 ${:.0f},  Q2 ${:.0f},  High Outlier ${:.0f},  Max ${:.0f}\"\n",
    "                                              .format(col, df_min, outliers_below,\n",
    "                                              Q1, Q3, outliers_above, df_max))\n",
    "    \n",
    "    print(\"{} IQR is ${:.0f}\".format(col,IQR))\n",
    "    \n",
    "    df_filtered = df.loc[((df[col] < outliers_above) & (df[col] > outliers_below))]\n",
    "\n",
    "    print(\"New df size after filter{},  Original df size {} \\n\\n\".format(df_filtered.shape, df.shape))\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sqft_lot = filter_outliers(seattle_df, \"sqft_lot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sqft_lot.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_filtered = filtered_sqft_lot  # Accept the filters\n",
    "seattle_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Perform visualizations and re-shape/transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the histograms of all features to decide which are continuous and which can be considered categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = seattle_filtered.hist(figsize = (12,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from .hist()\n",
    "\n",
    "## hist() helps determine which features are likely categorical and which are likely continuous\n",
    "\n",
    "* Categoricals include: view, waterfront, yr_renovated, condition, floors, possibly bedrooms and bathrooms, zipcode, yr_built\n",
    "* Continuous variables include:  lat, long, sqft_living (and all other sqft features)\n",
    "\n",
    "## hist() also helps show which features have outliers (long tails in the distribution)\n",
    "\n",
    "* Outliers on continuous features tend to distort models.  Because hist() is auto-scaling the x-axis, if there is \"nothing shown\" at the right or left of the axis,  it indicates there there is data at thos points,  but very few data points.\n",
    "\n",
    "* Examples here are: bedrooms (there is one property with 33 bedrooms), bathrooms, sqft_living (and related sqft metrics, and price.  Price is the target,  we will look at it because it seems to have a huge distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review feature correlation\n",
    "\n",
    "Looking at a correlation matrix reveals features which are potentially over-correlated (causing co-linearity)\n",
    "When a correlation of ~0.75 or greater is found between features, it is often best to remove one of the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run a Correlation matrix\n",
    "sns.axes_style(\"white\")\n",
    "\n",
    "pearson = seattle_filtered.corr(method = 'pearson')\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,12)})\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(pearson)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "ax = sns.heatmap(data=pearson, mask=mask, cmap=\"YlGnBu\", \n",
    "                 linewidth=0.5, annot=True, square=True, cbar_kws={'shrink': 0.5})\n",
    "\n",
    "plt.savefig(\"Correlation.png\")\n",
    "plt.savefig(\"Correlation 2.png\", transparent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of .corr()\n",
    "\n",
    "* Pairs of features that have higher correlation (> 0.75)\n",
    "    \n",
    "    * sqft_living and sqft_above - 0.84  - Unsurprising.  sqft_living is comprised of sqft_above and sqft_basement, but only ~35% of houses have basements\n",
    "    \n",
    "    * sqft_living and sqft_living15  - 0.7 - The information provided say that this sqft_living15 is the living space of the adjacent 15 houses.  This makes sense when there is a lot of community tract houses, built at the same time, in the same location.  However, generally speaking this doesn't make sense.\n",
    "    \n",
    "    * sqft_living and grade - 0.7 - Makes sense since grade is an assessment made which includes sqft_living\n",
    "    \n",
    "    * sqft_above and grade - 0.69 - because sqft_living and sqft_above are strongly correlated\n",
    "    \n",
    "#### Based on this analysis, I recommend removing sqft_above from the model in order to reduce co-linearity. However, to show this impact, we will leave these features in the dataframe and remove them at the time of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at some visualizations to see where our data is coming from geo-spatially, which may suggest good questions to ask\n",
    "\n",
    "### Let's look at location\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"map showing key zips.gif\" width=\"300\"/> <img src=\"kde on map 2.gif\" width=\"300\"/> \n",
    "</div\n",
    "\n",
    "Note: The KDE map was manually overlaid to the physical King County map.   A future work will be to see how to do the overlay of these plots using a software package like Bokeh or Tableau,  which is outside the scope of this project.\n",
    "\n",
    "\n",
    "\n",
    "### As mentioned, our audience is comprised of the top 1% of real estate brokers in King County.  A natural question to ask is whether the analysis pertains to their specific territory or region.   The KDE shows the vast majority of the sales data coming from the coastal region.   Thus, if brokers in the audience are selling properties inland,  the results of this analysis may not pertain to their territories. \n",
    "\n",
    "### Showing the audience where the data comes from,  provides a meaningful point of reference based on the audience's background, and helps establish expectations for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![Zipcode Geo-Spatial map](zipcodes_WestKC.gif \"Zipcodes in King County\")\n",
    "#https://aqua.kingcounty.gov/gis/web/VMC/boundaries/zipcodes/zipcodes.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.jointplot(x=seattle_df[\"long\"], y=seattle_df[\"lat\"], kind='kde', cmap=\"Blues\", \n",
    "              shade=True, shade_lowest=True);\n",
    "\n",
    "ax.set_title(\"What part of King County is the Model Based on?\");\n",
    "ax.set_xlabel(\"Latitude\");\n",
    "ax.set_ylabel(\"Longitude\");\n",
    "\n",
    "plt.savefig(\"kde jointplot transparent.png\", transparent = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3D Geo-spatial visualization using the longitude and latitude data\n",
    "\n",
    "*  This style of visualization allows 2-D and 3-D visualization of the distribution and density of our data (KDE, or Kernel Density Estimate) based on latitude and longitude. The plot is a non-parameterized way to estimate the probability density function of the data.  \n",
    "\n",
    "*  In plain English, this plot shows us where most of the house sales were on the map (based on lat and long).  This tells us where the majority of the data comes from.  The darker the hue, the more home sales in that area.\n",
    "\n",
    "*  From the standpoint of interpretability, I use this plot to tell realtors that our model is based on data from the most popular part of King County,  where there is a lot of turnover of house inventory.\n",
    "\n",
    "*  The joint plots are the slice of the probability density, at the maximum density, and show the shape of the probability density of sales. The shape shown is \"hilly\"  which is because that part of the county, the landmass is not continuous,  but is broken up numerous large and small bodies of water (where few or no houses have been sold).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to do 3-D plot is to create a scatter plot  of longitude and latitude, binning by a 3rd axis,  such as price, year-built, or sqft_living.\n",
    "\n",
    "* Below we show the results for the above, plus also the calculated price per square foot PPSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to plot is to characterize continuous variables in deciles using the .qcut()\n",
    "# .qcut() refers to quantiles.  In the case of deciles, we have q = 10, (0, 0.1, 0.2, 0.3....0.8, 0.9, 1.0)\n",
    "# For q = 4,  quartiles (0, 0.25, 0.5, 0.75, 1)\n",
    "\n",
    "seattle_df['Price %'] = pd.qcut(seattle_df['price'], q=10, labels=list(range(10))).astype(int)\n",
    "seattle_df['Year Built %'] = pd.qcut(seattle_df['yr_built'], q=10, labels=list(range(10))).astype(int)\n",
    "seattle_df['Square Foot Living Space %'] = pd.qcut(seattle_df['sqft_living'], q=10, labels=list(range(10))).astype(int)\n",
    "seattle_df['Price Per Square Foot %'] = pd.qcut(seattle_df['ppsf'], q=10, labels=list(range(10))).astype(int)\n",
    "\n",
    "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the scatter plot categorizing the data by percentiles \n",
    "\n",
    "def scatter_map(df, col_name, axes):\n",
    "    df.plot(kind='scatter', x='long', y='lat', c=col_name, \n",
    "            cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.1, ax=axes)\n",
    "    axes.set(xlabel='Longitude', ylabel='Latitude')\n",
    "    axes.set_title(col_name, fontsize=14)\n",
    "    return axes\n",
    "       \n",
    "fig, axes = plt.subplots(2, 2, sharey=True, sharex=True, figsize=(8, 6), dpi=100)\n",
    "\n",
    "scatter_map(seattle_df, 'Price %', axes.flat[0]);\n",
    "scatter_map(seattle_df, 'Year Built %', axes.flat[1]);\n",
    "scatter_map(seattle_df, 'Square Foot Living Space %', axes.flat[2]);\n",
    "scatter_map(seattle_df, 'Price Per Square Foot %', axes.flat[3]);\n",
    "fig.suptitle('Housing Metrics as a Function of Location', fontsize=15);\n",
    "#fig.delaxes(axes.flat[3])\n",
    "fig.tight_layout();\n",
    "fig.subplots_adjust(top=0.9);\n",
    "plt.show();\n",
    "plt.savefig(\"scatterplot.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  These 3-D visuals show there is a systematic price per square foot trend to location\n",
    "\n",
    "* The highest prices (shown in red on the scatter plots,  and in green on the map images) are on the shores of Lake Washington and the Puget Sound, closest to Seattle, where the views of both the water and the city of seattle are likely to be very good.   The prices are lower towards the south of Lake Washington. \n",
    "\n",
    "* Newer housing has been built, mostly away from the direct waterfront areas, inland, and to the south\n",
    "\n",
    "* Larger houses are essentially along Lake Washington facing West, towards Seattle, inland, and south,  corresponding to newer builds.\n",
    "\n",
    "* The price per square foot shows that housing prices are the highest radiating North of Seattle and to the East,  not only at the coast, but throughout the peninsula.  Very high price per square foot above Seattle, given that the house sqft_living is smaller in that area, and the houses are generally older.\n",
    "\n",
    "## The systematic price per square foot trend to latitude (i.e. the colors are consistently the same, within an area and not so splotchy)   is  unexpected finding\n",
    "\n",
    "### This suggests that there is a pricing rationale at work.   Despite other variables, such as age, the price normalized by square foot is very consistent by region,  which suggests houses prices are sold using price per square foot as the primary metric,  rather than a more complicated model.\n",
    "\n",
    "### Our audience which is comprised of the top 1% of brokers in the region, will likely accept the data as being solid, and by association,  the conclusions may be more readily accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some of the changes permanent\n",
    "\n",
    "seattle_df.drop(['Price %', 'Year Built %',\n",
    "       'Square Foot Living Space %', 'Price Per Square Foot %'],\n",
    "            axis = 1, inplace = True)\n",
    "seattle_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because pricing or price per square foot, is so strongly influeced by latitude (location), we should tae a closer look at Price vs. Zipcode using Boxplots\n",
    "## In particular, let's try to anticipate the questions our audience (realtors) might have regarding price and location\n",
    "\n",
    "1. Is there any significant trends between zipcode and price ?\n",
    "2. Is there any good way to group price trends?\n",
    "3. What's the home price impact of having waterfront housing ?\n",
    "4. Is there a significant difference between houses on the peninsula versus away from the coastline ?\n",
    "\n",
    "\n",
    "* Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html\n",
    "* Compare to the Pandas dateaframe, and you will see that the labelling in Matplotlib is done correctly\n",
    "\n",
    "* Be very careful -- using an \"equivalent\" code in Seaborn will lead to a wrong x-axis labelling\n",
    "    * Example shown later on below, with validation using the Pandas groupby\n",
    "* Since Seaborn doesn't throw an error,  it's easy to be mis-lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yellow = dict(markerfacecolor='y')\n",
    "sns.set(rc={'figure.figsize': (16,12)})\n",
    "seattle_filtered.boxplot(by='zipcode', column='price', fontsize = 12, rot = 90, flierprops = yellow);\n",
    "\n",
    "plt.title('Boxplots of Price Variation ($) Grouped by Zipcode', fontsize = 16)\n",
    "\n",
    "plt.xlabel('Zipcodes in King County', fontsize = 12);\n",
    "plt.ylabel('Price, $', fontsize = 12);\n",
    "plt.savefig(\"Price vs Zip.png\")\n",
    "\n",
    "\n",
    "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As data scientists,  we may not be domain experts in real estate.  Getting someone else's buy-in means learning something about another persons' reality so as to make a better \"connection\".  \n",
    "\n",
    "## Just like a best practice when you go to a foreign country to is learn a few words like \"hello\", \"please\" \"thank you\",  and \"excuse me\" will a long way to helping others relate, a best practice in data science is to think about real estate basics,  such as location,  to connect with the audience.\n",
    "\n",
    "## Go onto Google and pull up the King County zip code map and see what you can see. King county extends off to the east, but this picture is just the coastal region,  since that's where all the data comes from.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"key zips enlarged.gif\" width=\"300\"/>\n",
    "</div\n",
    "\n",
    "\n",
    "##  \n",
    "## We notice that the peninsula (outlined in black) has zip codes all starting with '981XX',  and the other zipcodes in the county are '980XX' zipcodes.\n",
    "\n",
    "## Some Conclusions from Prices vs. Zipcode\n",
    "\n",
    "1. Is there any significant trends between zipcode and price ?\n",
    "2. Is there any good way to group price trends?\n",
    "\n",
    "#### 3 zipcodes (98004, 98039, 98040) seem to have medians values much greater than 1 Million dollars. \n",
    "#### The map shows the reason why:   these are very special geographical areas (see green outline)\n",
    "#### 98004 - East bank of Lake Washington with prime views of downtown Seattle\n",
    "#### 98039 - Island of Medina on Lake Washington\n",
    "#### 98040 - Mercer Island, on Lake Washington\n",
    "###  \n",
    "\n",
    "\n",
    "### Investigate to see if there are price trends based on zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Separate out the 981XX and 980XX zipcode data\n",
    "zip_cut = np.where((seattle_df['zipcode']) >= 98100 , '981XX', '980XX')\n",
    "seattle_df.groupby(zip_cut)['price'].describe().rename(columns = {'count': '# of Houses', 'mean': 'Mean Price $',\n",
    "                                                                 'std': 'Price Variation $', 'min': 'Min Price $',\n",
    "                                                                 '25%': '1st Quartile $', '50%': 'Median Price $',\n",
    "                                                                 '75%': '3rd Quartile $', 'max': 'Max Price $'})\n",
    "\n",
    "# Conclusion:  These two populations look very similar -- but the 980XX population includes 3 zipcodes\n",
    "# which are very different :  98004, 98039, 98040  when you look at the boxplot above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for dropping based on condition\n",
    "# https://stackoverflow.com/questions/13851535/delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression-involving\n",
    "seattle_new = seattle_filtered.drop(seattle_filtered[(seattle_filtered.zipcode==98004) | (seattle_filtered.zipcode==98039) | \n",
    "                                         (seattle_filtered.zipcode==98040)].index)\n",
    "zip_cut = np.where((seattle_new['zipcode']) >= 98100,'981XX', '980XX')\n",
    "seattle_new.groupby(zip_cut)['price'].describe().rename(columns = {'count': '# of Houses',\n",
    "                                                                  'mean': 'Mean Price $', 'std': 'Price Variation $',\n",
    "                                                                  'min': 'Min Price $', '25%': '1st Quartile $',\n",
    "                                                                  '50%': 'Median Price $', '75%': '3rd Quartile $',\n",
    "                                                                  'max': 'Max Price $'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two groupings of zipcodes have very similar pricing distributions.  \n",
    "\n",
    "## Houses in the 981XX zipcode have a median house price difference of 50,000 dollars over those in the 980XX zips (excluding special areas)\n",
    "\n",
    "## Thus location by zip ~ 10% differential in median price\n",
    "* Removal of the 98004, 98039, and 98040 zip makes a big difference for the max price\n",
    "\n",
    "* Consider keeping these two datasets separate -- create two different datasets\n",
    "* Future work:  add a column with a label to indicate different regional groups such as those in the 981XX and 980XX zip codes, as well as those in special super high value real estate areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The home price impact by having waterfront \n",
    "\n",
    "* Less than 1% of houses have waterfront access\n",
    "* Median house price per square foot can be more than double those without waterfront\n",
    "* Median house prices can almost triple, depending on other factors, such as lot size and other factors\n",
    "\n",
    "## The analysis is shown below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seattle_df.groupby('waterfront')['ppsf','price'].describe().rename(columns = {'count': '# of Houses',\n",
    "                            'mean': 'Mean', 'std': 'Variation', 'min': 'Min', \n",
    "                            '25%': '1st Qtl','50%': 'Median', '75%': '3rd Qtl',\n",
    "                            'max': 'Max', 'ppsf': 'Price Per Sq Ft $/SF', 'price': 'Price $'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_filtered.groupby('waterfront')['ppsf','price'].describe().rename(columns = {'count': '# of Houses',\n",
    "                            'mean': 'Mean', 'std': 'Variation', 'min': 'Min', \n",
    "                            '25%': '1st Qtl','50%': 'Median', '75%': '3rd Qtl',\n",
    "                            'max': 'Max', 'ppsf': 'Price Per Sq Ft $/SF', 'price': 'Price $'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize': (6,5)})\n",
    "boxplot = sns.boxplot(x=seattle_df.waterfront, y=seattle_df.price, \n",
    "                      width=0.5, palette=\"Reds\", );\n",
    "\n",
    "boxplot.set_xticklabels(labels = ['No Waterfront', \"Waterfront\"],);\n",
    "boxplot.axes.set_title(\"Comparison of house prices in King County with and without waterfront)\",\n",
    "                    fontsize=16)\n",
    " \n",
    "boxplot.set_xlabel(\"Comparison of Home Prices Without and With Waterfront \", \n",
    "                fontsize=14)\n",
    " \n",
    "boxplot.set_ylabel(\"Price, '$'\",\n",
    "                fontsize=14)\n",
    " \n",
    "boxplot.tick_params(labelsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the price distributions on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot price and price per square foot distributions\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "title = fig.suptitle(\"Home Price Distributions in King County (After filtering lot and living areas)\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_title(\"Price Distribution\", fontsize=12)\n",
    "ax1.set_ylabel(\"Distribution A.U.\",)\n",
    "sns.distplot(seattle_df['price'], ax=ax1, color='b', axlabel='Price $');\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_title(\"Price Per Sq Ft Distribution\",)\n",
    "ax2.set_ylabel(\"Distribution A.U.\", ) \n",
    "sns.distplot(seattle_df['ppsf'], ax=ax2, color='r', axlabel = \"Price Per Sq Ft $/SqFt\");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Distribution is non-normal and still has a long tail\n",
    "\n",
    "* This plot is auto-scaled and so suggests that there are outliers in the data which were also seen in the boxplot\n",
    "* We can see if applying a filter on the price distribution is of any help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "filtered_price = filter_outliers(seattle_df, 'price')\n",
    "filtered_ppsf = filter_outliers(seattle_df, 'ppsf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Calculation of Interquartile Means\n",
    "* Because of the non-normality of the distribution and non-centered mean value, the calculation of the cutoff for outliers goes to negative values.\n",
    "* Thus no data will be filtered from the low end of price\n",
    "\n",
    "## Take a look at the price distribution, post filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot price and price per square foot distributions\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "title = fig.suptitle(\"Price Distributions in King County After Removing Outliers\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_title(\"Price Distribution\")\n",
    "#ax1.set_xlabel(\"Prices $\")\n",
    "ax1.set_ylabel(\"Distribution A.U.\") \n",
    "sns.distplot(filtered_price['price'], ax=ax1, color='b', axlabel='Price $');\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_title(\"Price Per Sq Ft Distribution\")\n",
    "#ax2.set_xlabel(\"Price Per Sq Foot $/Sq Ft\")\n",
    "ax2.set_ylabel(\"Distribution A.U.\") \n",
    "sns.distplot(filtered_ppsf['ppsf'], ax=ax2, color='r', axlabel='Price per Sq Ft $/Sq Ft');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Observations\n",
    "1. Higher prices at higher latitudes, and longitudes below -122.0\n",
    "\n",
    "2. On the face of it, the zipcode is not a good predictor of pricing,  -- there is roughly a 10% difference in median home prices between the 981XX zip code and the 980XX zip code.\n",
    "\n",
    "3. Waterfront commands roughly 2X increase in Median Price Per Square Foot.  Median house prices can increase almost 3X depending on other factors. \n",
    "\n",
    "## Modeling comparisons\n",
    "\n",
    "## We have several datasets which have been cleaned differently to see what is the impact of cleaning on linear regression modelling\n",
    "\n",
    "1. seattle_df  - original dataset with Nans, wrong data inputed,  and datatypes converted as needed\n",
    "2. seattle_filtered - filtering outliers based on sqft_lot and then sqft_living\n",
    "3. filtered_price - filtering price outliers from seattle_df(1.)\n",
    "4. filtered_ppsf - filtering ppsf outliers from seattle_df (1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Multi-Linear Regression Model\n",
    "## Modelling Strategy\n",
    "\n",
    "### 1.  In most cases, one would split the data into a train and test set. We will leave that to future work.\n",
    "\n",
    "#### However, in this case we are just going to stick with training all the data\n",
    "\n",
    "### 2. Encoding, Log Transformation, and Normalization of categoricals and feature normalization of continuous variables, for all cases.\n",
    "\n",
    "#### In this case we will include the code and function definitions for encoding of categoricals and continuous variables,  but only perform this on the \"best\" model data based on the cases above.\n",
    "\n",
    "### 3. Perform step-wise feature selection\n",
    "\n",
    "### 4.  Analyse the model results\n",
    "    * Review the features p-values and ensure that the null-hypothesis is not true\n",
    "    * Review the adjusted accuracy.  Generally the higher the better\n",
    "    * Review the Condition Number  -- Normally <20 is desirable,  which means that the features are not co-linear\n",
    "    * Revew the Skew and Kurtosis of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.5, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    This function comes from the Flatiron School curriculum\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Work - automate this portion of the analysis\n",
    "# Had difficulty creating a single function call that would perform the feature selection and model at the same time\n",
    "\n",
    "data_1 = seattle_df.drop(columns = ['price','sqft_living15', 'sqft_lot15', 'sqft_above', 'ppsf',])\n",
    "data_2 = seattle_filtered.drop(columns = ['price','sqft_living15', 'sqft_lot15', 'sqft_above', 'ppsf',])\n",
    "data_3 = filtered_price.drop(columns = ['price','sqft_living15', 'sqft_lot15', 'sqft_above', 'ppsf',])\n",
    "data_4 = filtered_ppsf.drop(columns = ['price','sqft_living15', 'sqft_lot15', 'sqft_above', 'ppsf',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.shape, data_2.shape, data_3.shape, data_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_1\n",
    "y = pd.DataFrame(seattle_df, columns = ['price'])\n",
    "res1 = stepwise_selection(X, y, verbose = False)\n",
    "\n",
    "X = data_2\n",
    "y = pd.DataFrame(seattle_filtered, columns = ['price'])\n",
    "res2 = stepwise_selection(X, y, verbose = False)\n",
    "\n",
    "X=data_3\n",
    "y = pd.DataFrame(filtered_price, columns = ['price'])\n",
    "res3 = stepwise_selection(X, y, verbose = False)\n",
    "\n",
    "X=data_4\n",
    "y = pd.DataFrame(filtered_ppsf, columns = ['price'])\n",
    "res4 = stepwise_selection(X, y, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the OLS model with the features selected and stored in results\n",
    "X_fin = X[res1]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model1 = sm.OLS(y, X_with_intercept).fit()\n",
    "model1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_fin = X[res2]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model2= sm.OLS(y, X_with_intercept).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_fin = X[res4]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model4 = sm.OLS(y, X_with_intercept).fit()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_fin = X[res3]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model3 = sm.OLS(y, X_with_intercept).fit()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "###  The Adjusted R2 accuracy is 0.738.  However, the condition number shows the features are highly co-linear\n",
    "\n",
    "###  All of the p-values are smaller than 0.05\n",
    "\n",
    "###  Skew and Kurtosis could be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Encoding and Normalization Procedures\n",
    "\n",
    "* Normalization could change the accuracy slightly,  and it is supposed to improve the skew and kurtosis\n",
    "* Encoding may improve the accuracy by allowing the linearity / normality of the data to be more important than the labeling of the data.  However, from working with this particular data,  encoding isn't playing a significant role.\n",
    "\n",
    "This section is about re-binning for categorical features and performing hot-encoding. \n",
    "\n",
    "We have decided based on the ETL that the categoricals we can use are: waterfront, floors, view, condition, grade, yr_built, yr_renovated, zipcode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = seattle_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.drop(['price','bedrooms', 'sqft_living', 'sqft_lot', 'sqft_basement', 'yr_built',\n",
    "            'lat', 'long', 'sqft_above','sqft_living15','sqft_lot15', 'ppsf'],\n",
    "            axis = 1, inplace = True)\n",
    "\n",
    "# Drop all continuous variables and other intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.bathrooms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bathrooms need to be categorized;  appears to be some typos in the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.bathrooms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1.75, 2.25, 3, 8]\n",
    "bins_bathrooms = pd.cut(seattle_df['bathrooms'], bins)\n",
    "bins_bathrooms.cat.as_ordered()\n",
    "bins_bathrooms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins_bathrooms.value_counts().plot(kind = 'bar', figsize = (3,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['bathrooms'] = bins_bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.floors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1.5, 2, 4]\n",
    "bins_floors = pd.cut(seattle_df['floors'], bins)\n",
    "bins_floors.cat.as_ordered()\n",
    "bins_floors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins_floors.value_counts().plot(kind = 'bar', figsize = (3,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['floors'] = bins_floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.grade.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 6, 7, 8, 13]\n",
    "bins_grade = pd.cut(cat_df['grade'], bins)\n",
    "bins_grade = bins_grade.cat.as_ordered()\n",
    "bins_grade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins_grade.value_counts().plot(kind='bar', figsize=(3,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['grade']= bins_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['condition'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,3,5]\n",
    "bins_condition = pd.cut(cat_df['condition'], bins)\n",
    "bins_condition.cat.as_ordered()\n",
    "bins_condition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bins_condition.value_counts().plot(kind = 'bar', figsize =(3,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['condition']= bins_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['yr_renovated'] = cat_df['yr_renovated'].astype('category')\n",
    "cat_df['yr_renovated'] = cat_df['yr_renovated'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['zipcode'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 98050, 98075, 98199]\n",
    "bins_zipcode = pd.cut(cat_df['zipcode'], bins)\n",
    "bins_zipcode.cat.as_ordered()\n",
    "bins_zipcode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bins_zipcode.value_counts().plot(kind='bar', figsize = (3,3));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['zipcode'] = bins_zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to categoricals\n",
    "cat_df.waterfront = cat_df.waterfront.astype('category')\n",
    "cat_df.view = cat_df.view.astype('category')\n",
    "cat_df.yr_renovated = cat_df.yr_renovated.astype('category')\n",
    "cat_df.floors = cat_df.floors.astype('category')\n",
    "cat_df.bathrooms = cat_df.bathrooms.astype('category')\n",
    "cat_df.condition = cat_df.condition.astype('category')\n",
    "cat_df.grade = cat_df.grade.astype('category')\n",
    "cat_df.zipcode = cat_df.zipcode.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding to categoricals\n",
    "\n",
    "cat_df['waterfront'] = cat_df['waterfront'].cat.codes\n",
    "cat_df['bathrooms'] = cat_df['bathrooms'].cat.codes\n",
    "cat_df['yr_renovated']= cat_df['yr_renovated'].cat.codes\n",
    "cat_df['view'] = cat_df['view'].cat.codes\n",
    "cat_df['grade'] = cat_df['grade'].cat.codes\n",
    "cat_df['condition'] = cat_df['condition'].cat.codes\n",
    "cat_df['zipcode'] = cat_df['zipcode'].cat.codes\n",
    "cat_df['floors'] = cat_df['floors'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bathrooms_dummy = pd.get_dummies(bins_bathrooms, prefix = \"BATH\", drop_first = True)\n",
    "floors_dummy = pd.get_dummies(bins_floors, prefix = 'FLR', drop_first = True)\n",
    "conditions_dummy = pd.get_dummies(bins_condition, prefix = \"CON\", drop_first = True)\n",
    "grade_dummy = pd.get_dummies(bins_grade, prefix= 'YRBLT', drop_first = True)\n",
    "zipcode_dummy = pd.get_dummies(bins_zipcode, prefix = 'ZIP', drop_first = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now check to see if transformations (such as log transform) help to improve the normality of continuous data\n",
    "\n",
    "* The continuous variables are: bedrooms, bathrooms, sqft_living, sqft_lot, sqft_above, sqft_basement, yr_built, long,\n",
    "  lat,\n",
    "* I'm dropping off some data columns which were shown to have high correlation with sqft_living (sqft_living15)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare to check for log transformations on continuous data\n",
    "cont_df = seattle_df.copy()\n",
    "\n",
    "cont_df.drop(['price','floors','waterfront', 'bathrooms','view', 'grade',\n",
    "               'sqft_above', 'zipcode', 'ppsf', 'sqft_living15' ],\n",
    "            axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out log transformations\n",
    "data_log = pd.DataFrame([])\n",
    "#data_log['bathrooms'] = np.log(cont_df['bathrooms'])\n",
    "data_log['bedrooms'] = np.log(cont_df['bedrooms'])\n",
    "data_log['lat'] = np.log(cont_df['lat'])\n",
    "\n",
    "# data_log['long'] = np.log(cont_df['long'])  - due to negative numbers\n",
    "\n",
    "data_log['sqft_living'] = np.log(cont_df['sqft_living'])\n",
    "data_log['sqft_lot'] = np.log(cont_df['sqft_lot'])\n",
    "data_log['sqft_lot15'] = np.log(cont_df['sqft_lot15'])\n",
    "#data_log['sqft_basement'] = np.log(cont_df['sqft_basement']) # too many zeros\n",
    "\n",
    "data_log['yr_built'] = np.log(cont_df['yr_built'])\n",
    "\n",
    "data_log.hist(figsize = [8,8]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Observation:  The only features really improved by log transformations are the sqft_living, so keep the transformation for those features\n",
    "\n",
    "## Now perform scaling or normalization on continuous variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# Use min-max normalization on log transformed features:  log_sqft_living, log_sqft_living15, log_sqft_above\n",
    "# Use min-max normalization on sqft_basement, sqft_lot, sqft_lot15\n",
    "# Use mean normalization on the bedrooms and bathrooms\n",
    "# Use standardization on yr_built, lat, long\n",
    "\n",
    "#bathrooms = cont_df['bathrooms']\n",
    "bedrooms = cont_df['bedrooms']\n",
    "log_sqft_living = data_log['sqft_living']\n",
    "\n",
    "\n",
    "sqft_basement = cont_df['sqft_basement']\n",
    "sqft_lot = data_log['sqft_lot']\n",
    "sqft_lot15 = data_log['sqft_lot15']\n",
    "\n",
    "yr_built = cont_df['yr_built']\n",
    "lat = cont_df['lat']\n",
    "long = cont_df['long']\n",
    "\n",
    "# Mean Normalization\n",
    "#scaled_bathrooms = (bathrooms - np.mean(bathrooms)) / (max(bathrooms) - min(bathrooms))\n",
    "scaled_bedrooms = (bedrooms - np.mean(bedrooms)) / (max(bedrooms) - min(bedrooms))\n",
    "\n",
    "\n",
    "# Min-Max Scaling on log transformed data\n",
    "scaled_logsqft_liv = (log_sqft_living - min(log_sqft_living)) / (max(log_sqft_living) - min(log_sqft_living))\n",
    "\n",
    "# Min-Max Scaling on non-log transformed features:  sqft_basement\n",
    "scaled_sqft_basement = (sqft_basement - min(sqft_basement)) / (max(sqft_basement)- min(sqft_basement))\n",
    "scaled_sqft_lot = (sqft_lot - min(sqft_lot)) / (max(sqft_lot)- min(sqft_lot))\n",
    "scaled_sqft_lot15 = (sqft_lot15 - min(sqft_lot15)) / (max(sqft_lot15)- min(sqft_lot15))\n",
    "\n",
    "# Standardization\n",
    "scaled_yrbuilt = (yr_built - np.mean(yr_built)) / np.sqrt(np.var(yr_built))\n",
    "scaled_lat = (lat - np.mean(lat)) / np.sqrt(np.var(lat))\n",
    "scaled_long = (long - np.mean(long)) / np.sqrt(np.var(long))\n",
    "\n",
    "cont_df2 = pd.DataFrame([])\n",
    "cont_df2['bedrooms'] = scaled_bedrooms\n",
    "#cont_df2['bathrooms'] = scaled_bathrooms\n",
    "\n",
    "cont_df2['sqft_living'] = scaled_logsqft_liv\n",
    "\n",
    "cont_df2['sqft_basement'] = scaled_sqft_basement\n",
    "cont_df2['sqft_lot'] = scaled_sqft_lot\n",
    "cont_df2['sqft_lot15'] = scaled_sqft_lot15\n",
    "\n",
    "cont_df2['yr_built'] = scaled_yrbuilt\n",
    "cont_df2['lat'] = scaled_lat\n",
    "cont_df2['long'] = scaled_long\n",
    "\n",
    "price = seattle_df['price']\n",
    "df_final = pd.concat([price, cat_df, cont_df2], axis=1, sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building after normalization on seattle_df dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns   # df_final is the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = df_final.drop(columns = ['price', 'sqft_lot15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_model\n",
    "y = pd.DataFrame(df_final, columns = ['price'])\n",
    "\n",
    "result = stepwise_selection(X, y, verbose = True)\n",
    "\n",
    "print(\"Resulting features\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_fin = X[result]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model = sm.OLS(y, X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfortunately, normalization didn't much help the model accuracy.\n",
    "## None of the feature p-values are > 0.05\n",
    "## The Collinearity improved.\n",
    "\n",
    "## The top 5 contributors to the model have been latitude, grade, living area, view and age of the home (year built)\n",
    "\n",
    "## Let's look a little deeper into grade  to see what kind of price impact we can see,  A homeowner can potentially make improvements to the home or land, to improve the Grade score - so let's try to see what they are worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with grade,  it might be the easier one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df.groupby('grade')['price','ppsf'].describe().rename(columns = {'count': '# of Houses',\n",
    "                            'mean': 'Mean', 'std': 'Variation', 'min': 'Min', \n",
    "                            '25%': '1st Qtl','50%': 'Median', '75%': '3rd Qtl',\n",
    "                            'max': 'Max', 'ppsf': 'Price Per Sq Ft $/SF', 'price': 'Price $'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the one house that is a grade 3, and may be a typo\n",
    "seattle_df.sort_values(by=['grade']).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_df = seattle_df.drop(seattle_df[seattle_df.grade == 3].index)\n",
    "seattle_df.shape\n",
    "#reference: https://stackoverflow.com/questions/13851535/delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression-involving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = seattle_df.groupby('grade')['price', 'ppsf'].median()\n",
    "new_df = seattle_df[['grade', 'price', 'ppsf']].copy()\n",
    "\n",
    "new_df.groupby('grade')['price','ppsf'].describe().rename(columns = {'count': '# of Houses',\n",
    "                            'mean': 'Mean', 'std': 'Variation', 'min': 'Min', \n",
    "                            '25%': '1st Qtl','50%': 'Median', '75%': '3rd Qtl',\n",
    "                            'max': 'Max', 'ppsf': 'Price Per Sq Ft $/SF', 'price': 'Price $'})\n",
    "\n",
    "#reference: https://stackoverflow.com/questions/34682828/extracting-specific-selected-columns-to-new-dataframe-as-a-copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will drop out that value\n",
    "# Then do a bar chart for both price and price per square foot\n",
    "# Visualization to Determine Ways to Condition the Data\n",
    "# References: \n",
    "\n",
    "# https://jovianlin.io/data-visualization-seaborn-part-2/\n",
    "\n",
    "# https://jovianlin.io/data-visualization-seaborn-part-3/\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "title = fig.suptitle(\"Grade Value for Homes in King County\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.35, )\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_title(\"Impact of Grade Value on Price\", fontsize=12)\n",
    "#ax1.tick_params(labelsize=12)\n",
    "sns.barplot(x='grade', y='price', data=new_df.reset_index(), ax=ax1);\n",
    "ax1.set_ylabel(\"Median Price $\",)\n",
    "ax1.set_xlabel(\"Grade\")\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_title(\"Impact of Grade Value on Price per Sq Foot\",)\n",
    "\n",
    "sns.barplot(x='grade', y='ppsf', ax=ax2, data=new_df.reset_index(),);\n",
    "ax2.set_ylabel(\"Median Price Per Sq Ft($ / Sqft)\");\n",
    "ax2.set_xlabel(\"Grade\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The King County Grade can have an impact on both Median Home Price as well as Median Price Per Sq Ft.  \n",
    "\n",
    "### Median Home Price appears to scale monotonically with the Grade. What’s a bit more interesting is that the Median Price Per Sq. Foot doesn’t scale monotonically with grade.\n",
    "\n",
    "### Approximate doubling of the median home prices from grade '5.5 to 8,  8 to 10, and from 10 to \"11.5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_new = seattle_filtered.drop(seattle_filtered[(seattle_filtered.zipcode==98004) | (seattle_filtered.zipcode==98039) | \n",
    "                                         (seattle_filtered.zipcode==98040)].index)\n",
    "zip_cut = np.where((seattle_new['zipcode']) >= 98100,'981XX', '980XX')\n",
    "seattle_new.groupby(zip_cut)['price'].describe().rename(columns = {'count': '# of Houses',\n",
    "                                                                  'mean': 'Mean Price $', 'std': 'Price Variation $',\n",
    "                                                                  'min': 'Min Price $', '25%': '1st Quartile $',\n",
    "                                                                  '50%': 'Median Price $', '75%': '3rd Quartile $',\n",
    "                                                                  'max': 'Max Price $'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = seattle_df[['zipcode', 'grade', 'price', 'ppsf']].copy()\n",
    "\n",
    "seattle_new = new_df2.drop(new_df2[(new_df2.zipcode==98004) | (new_df2.zipcode==98039) | \n",
    "                                   (new_df2.zipcode==98040)].index)\n",
    "\n",
    "zip_cut = np.where((seattle_new['zipcode']) >= 98100,'981XX', '980XX')\n",
    "\n",
    "seattle_new.groupby([zip_cut,'grade'])['price', 'ppsf'].describe().rename(columns = {'count': '# of Houses',\n",
    "                                                                  'mean': 'Mean', 'std': 'Variation',\n",
    "                                                                  'min': 'Min', '25%': '1st Qtl',\n",
    "                                                                  '50%': 'Median', '75%': '3rd Qtl',\n",
    "                                                                  'max': 'Max', 'price': \"Price  $\", \n",
    "                                                                   'ppsf': 'Price Per SqFt   $/SqFt'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting by zipcode shows that the curious feature of increased Median Price Per Square Foot at low Grades,  only on on the 980XX properties.\n",
    "\n",
    "## Future work as to why that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The trend of higher median price per sqft at lower grades is a trend primarily on the 980XX zipcodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn plots the \"Groupby\",  but messes up the zipcode label order\n",
    "# This problem was reported as a PR back in 2013,  and was supposed to be fixed/closed.\n",
    "# https://github.com/mwaskom/seaborn/issues/14\n",
    "\n",
    "# However, the current documentation is confusing and difficult to understand how to condition the data to \n",
    "# get it to display properly.  There is no discussion on the impact of not specifying\n",
    "# the label order,  since a plot with the wrong order of x labels is extremely misleading\n",
    "# https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "\n",
    "# Given that the boxplot itself is constructed properly by grouping together the data of the same zipcode\n",
    "# to get the quartiles, median, and outlier positions,  it would be far better that Seaborn would attach the correct \n",
    "# labels to the grouped values --as the default.  Not as something that to be done separately\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,12)})\n",
    "sns.set(style=\"whitegrid\")\n",
    "boxplot = sns.boxplot(x=seattle_df.zipcode, y=seattle_df.price, \n",
    "                      width=0.5, palette=\"Set3\");\n",
    "\n",
    "boxplot.set_xticklabels(labels = seattle_df.zipcode, rotation=90);\n",
    "boxplot.axes.set_title(\"Variation of Prices in King County by Zipcode (Wrong Zipcode Order, do not use!)\",\n",
    "                    fontsize=16)\n",
    " \n",
    "boxplot.set_xlabel(\"Zipcodes in King County -- Wrong Zipcode Order !!! \", \n",
    "                fontsize=14)\n",
    " \n",
    "boxplot.set_ylabel(\"Price, '$'\",\n",
    "                fontsize=14)\n",
    " \n",
    "boxplot.tick_params(labelsize=12)\n",
    "\n",
    "\n",
    "# According to Seaborn documentation,  need to put provide the x-axis labels as a list of strings.\n",
    "# However, the way I got to the list of strings (called x_axis) didn't seem to work\n",
    "#x_axis = seattle_df['zipcode'].astype(str).tolist()\n",
    "#sns.set(style=\"whitegrid\")\n",
    "#ax = sns.boxplot(x=seattle_df.zipcode, y=seattle_df.price, width=0.5, palette=\"Set3\", order=x_axis);\n",
    "#ax.set_xticklabels(x_axis, rotation=90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = seattle_df.groupby('zipcode')['price'].describe()\n",
    "test.head()\n",
    "\n",
    "# You can see that zipcode 98004 has a median of $1.15M and max of $7.06M\n",
    "# If you look at the Seaborn graph,  zipcode 98004 is the very far to the right,  and the median and max do not match!\n",
    "# if you look at the Pandas boxplot,  where the zipcodes are consecutive,  you find the data in the table matches that of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seattle Police Deparment Section 6.220 of the Police Manual\n",
    "\n",
    "http://www.seattle.gov/police-manual/title-6---arrests-search-and-seizure/6220---voluntary-contacts-terry-stops-and-detentions\n",
    "\n",
    "http://www.seattle.gov/police/information-and-data/terry-stops/terry-stops-dashboard\n",
    "\n",
    "Effective Date: 01/01/2020\n",
    "\n",
    "This policy applies to all sworn employees conducting voluntary contacts and/or stops/detentions based upon reasonable suspicion (Terry).\n",
    "\n",
    "This policy does not apply to detentions based upon probable cause and community caretaking functions pursuant to RCW 71.05.153.\n",
    "\n",
    "6.220 - POL – 1 Definitions\n",
    "\n",
    "Seizure: A seizure occurs any time an officer, by means of physical force or show of authority, restricts the liberty of a person.  A seizure may also occur if an officer uses words, actions, or demeanor that would make a reasonable person believe that they are not free to leave.\n",
    "\n",
    "Voluntary Contacts: During voluntary contacts, officers will not use any words, actions, demeanor, or other show of authority that would indicate that a person is not free to leave; voluntary contacts are not seizures.\n",
    "\n",
    "Voluntary Contacts fall under two categories:\n",
    "\n",
    "Social Contact: A voluntary and consensual encounter between the police and a subject with the intent of engaging in casual and/or non-investigative conversation. The subject is free to leave and/or decline any of the officer’s requests at any point; social contacts are not seizures.\n",
    "\n",
    "Non-Custodial Interview: A voluntary and consensual investigatory interview that an officer conducts with a subject during which the subject is free to leave and/or decline any of the officer’s requests at any point; non-custodial interviews are not seizures.\n",
    "\n",
    "Terry Stop: A brief, minimally intrusive seizure of a subject based upon articulable reasonable suspicion in order to investigate possible criminal activity. The stop can apply to people as well as vehicles. The subject of a Terry stop is not free to leave. A Terry stop is a seizure under both the state and federal constitutions.\n",
    "\n",
    "- A Terry stop is a detention, based on reasonable suspicion, during which an officer may develop facts to establish probable cause or dispel suspicion.\n",
    "\n",
    "- Stops and detentions initiated under probable cause will be made pursuant to Manual Sections:\n",
    "\n",
    "- 6.010- Arrests;\n",
    "\n",
    "- 6.280-Warrant Arrests;\n",
    "\n",
    "- 16.230-Issuing Tickets and Traffic Contact Reports;\n",
    "\n",
    "- 16.110-Crisis Intervention or;\n",
    "\n",
    "- 15.020 - Charge-By-Officer\n",
    "\n",
    "Reasonable Suspicion: Specific, objective, articulable facts, which, taken together with rational inferences, would create a well-founded suspicion that there is a substantial possibility that a subject has engaged, is engaging or is about to engage in criminal conduct.\n",
    "\n",
    "-The reasonableness of a Terry stop is considered in view of the totality of the circumstances, the officer’s training and experience, and what the officer knew before the stop.\n",
    "\n",
    "- During a stop, an officer may learn new information that can lead to additional reasonable suspicion or probable cause that a crime has occurred, but that new information cannot provide the justification for the original stop.\n",
    "\n",
    "6.220 - POL – 2 Conducting a Terry Stop\n",
    "\n",
    "1. Terry Stops are Seizures Based Upon Reasonable Suspicion\n",
    "\n",
    "This policy prohibits Terry stops when an officer lacks reasonable suspicion that a subject has been, is, or is about to engage in the commission of a crime.\n",
    "\n",
    "Searches and seizures by officers are lawful to the extent they meet the requirements of the 4th Amendment (see Terry v. Ohio, 392 U.S. 1 (1968), and Washington Constitution Art. 1, Section 7.\n",
    "\n",
    "2. During a Terry Stop, Officers Will Limit the Seizure to a Reasonable Scope\n",
    "\n",
    "Officers will articulate in their Report, the justification for the initiation, scope and duration of a Terry stop.\n",
    "\n",
    "Actions that would indicate to a reasonable person that they are under arrest or indefinitely detained may convert a Terry stop to an arrest; however, taking any of these actions does not necessarily turn a Terry stop into an arrest.\n",
    "\n",
    "Unless justified by the articulable reasons for the original stop, officers must have additional articulable justification for further limiting a person’s freedom during a Terry stop, such as:\n",
    "\n",
    "- Taking a subject’s identification or driver license away from the immediate vicinity\n",
    "\n",
    "- Ordering a motorist to exit a vehicle\n",
    "\n",
    "- Putting a pedestrian up against a wall\n",
    "\n",
    "- Directing a person to stand or remain standing, or to sit on a patrol car bumper or any other place not of their choosing\n",
    "\n",
    "- Directing a person to lie or sit on the ground\n",
    "\n",
    "- Applying handcuffs\n",
    "\n",
    "- Transporting any distance away from the scene of the initial stop, including for the purpose of witness identification\n",
    "\n",
    "- Placing a subject into a police vehicle\n",
    "\n",
    "- Pointing a firearm at a person or occupied vehicle\n",
    "\n",
    "- Frisking for weapons\n",
    "\n",
    "- De minimis force\n",
    "\n",
    "3. During a Terry Stop, Officers Will Limit the Seizure to a Reasonable Amount of Time\n",
    "\n",
    "Subjects may be seized for only that period of time necessary to effect the purpose of the stop. Any delays in completing the necessary actions will be objectively reasonable.\n",
    "\n",
    "Officers may not extend a detention solely to await the arrival of a supervisor.\n",
    "\n",
    "4. During all Terry Stops, Officers Will Take Reasonable Steps to Be Courteous and Professional\n",
    "\n",
    "When reasonable, as early in the contact as safety permits, the officer making contact with the subject (contact officer) will inform the suspect of the following:\n",
    "\n",
    "- The officer’s name;\n",
    "\n",
    "- The officer’s rank or title;\n",
    "\n",
    "- The fact that the officer is a Seattle Police Officer;\n",
    "\n",
    "- The reason for the stop; and\n",
    "\n",
    "- That the stop is being recorded, if applicable (See 16.090 – In-Car and Body Worn Video).\n",
    "\n",
    "When releasing a person at the end of a Terry stop, officers will advise the person that they are free to leave, offer an explanation of the circumstances and reasons for the Terry stop, and provide the person a business card with the event number as a receipt. Officers will not extend a detention to explain the Terry stop or provide a receipt.\n",
    "\n",
    "5. Officers Cannot Require Subjects to Identify Themselves or Answer Questions on a Terry Stop\n",
    "\n",
    "During a Terry stop, officers may request identification; however, subjects are not obligated to provide identification or information upon request.\n",
    "\n",
    "Exceptions: As listed in 6.220—POL-3 Conducting a Detention to Issue a Notice of Infraction, Issue a Citation, and Other Exceptions.\n",
    "\n",
    "6. Officers May Conduct a Frisk of Stopped Subject(s) Only if They Have an Articulable and Reasonable Safety Concern that the Person is Armed and Presently Dangerous\n",
    "\n",
    "The purpose and scope of a frisk is to discover weapons or other items which pose a danger to the officer or those nearby. It is not a generalized search of the entire person. The decision to conduct a frisk is based upon the totality of the circumstances and the reasonable conclusions drawn from the officer’s training and experience.  Generally, the frisk will be limited to a pat-down of outer clothing.  Once the officer ascertains that no weapon is present after the frisk is completed, the officer’s limited authority to frisk is completed (i.e. the frisk will stop).\n",
    "\n",
    "- A weapons frisk is a limited search determined by the state and federal constitutions.\n",
    "\n",
    "- All consent searches will be conducted and memorialized via body-worn video, in-car video or signed consent form pursuant to Manual Section 6.180.\n",
    "\n",
    "- Officers will not frisk for weapons on a social contact or noncustodial interview.\n",
    "\n",
    "- A frisk will not be used as a pretext to search for incriminating evidence.\n",
    "\n",
    "- The fact that a Terry stop occurs in a high-crime area is not by itself sufficient to justify a frisk.\n",
    "\n",
    "Frisk factors may include, but are not limited to:\n",
    "\n",
    "- Prior knowledge that the subject carries a weapon;\n",
    "\n",
    "- Suspicious behavior, such as failure to comply with instructions to keep hands in sight; and\n",
    "\n",
    "- Observations, such as suspicious bulges, consistent with carrying a concealed weapon.\n",
    "\n",
    "7. Under Washington State Law, Traffic Violations Will Not Be Used as a Pretext to Investigate Unrelated Crimes\n",
    "\n",
    "- Pretext is stopping a suspect for an infraction to investigate criminal activity for which the officer has neither reasonable suspicion nor probable cause.\n",
    "\n",
    "- The Washington State Constitution forbids use of pretext as a justification for a warrantless search or seizure.\n",
    "\n",
    "- Officers will consciously, and independently determine that a traffic stop is reasonably necessary in order to address a suspected traffic infraction.\n",
    "\n",
    "8. Supervisors Will Screen All Incidents In-Person When an Officer Places Handcuffs on a Subject\n",
    "\n",
    "Officers will not extend a detention solely to await the arrival of a supervisor.\n",
    "\n",
    "When un-handcuffing a subject for release, the officer will immediately notify a supervisor, inform the subject that they are free to leave and inform them that a sergeant is en route to the scene.\n",
    "\n",
    "- If the subject declines to speak with a supervisor or wishes to leave before the supervisor arrives, the officer will attempt to offer the subject the supervisor's contact information.\n",
    "\n",
    "- If the subject decides to wait for the supervisor, the officer will wait at the location for the supervisor to arrive.\n",
    "\n",
    "- If the subject does not wish to remain on-scene to speak with the supervisor, the officer may arrange to meet the supervisor at another location to screen the incident.\n",
    "\n",
    "9. When Making an Arrest, Officers May Seize Non-Arrested Companions for Articulable and Reasonable Officer Safety Concerns\n",
    "\n",
    "Officers will only maintain the seizure of non-arrested companions based on safety concerns for as long as the objective rationale for the seizure continues to exist.  The scope and nature of the seizure must be objectively reasonable based on the factors justifying the detention.\n",
    "\n",
    "Officers will articulate objective safety concerns for the officers, the arrestee, their companions, or other persons when seizing non-arrested companions.\n",
    "\n",
    "Factors to consider when seizing non-arrested companions include (but are not limited to):\n",
    "\n",
    "- The type of arrest;\n",
    "\n",
    "- The number of officers;\n",
    "\n",
    "- The number of people present at the scene of the arrest;\n",
    "\n",
    "- The time of day;\n",
    "\n",
    "- The behavior of those present at the scene;\n",
    "\n",
    "- The location of the arrest;\n",
    "\n",
    "- The presence or suspected presence of a weapon;\n",
    "\n",
    "- Officer knowledge of the arrestee or the companions; and/or\n",
    "\n",
    "- Potentially affected persons\n",
    "\n",
    "This is not an exhaustive list. Justification to detain non-arrested companions will be made based upon the totality of the circumstances.\n",
    "\n",
    "6.220 - POL – 3 Conducting a Detention to Issue a Notice of Infraction, Issue a Citation, and Other Exceptions\n",
    "\n",
    "1. Certain Statutory Exceptions Require the Subject to Provide Identification:\n",
    "\n",
    "- When the subject is a driver stopped for a traffic infraction investigation (RCW 46.61.021) failure to provide identification is a misdemeanor.\n",
    "\n",
    "- When the subject is attempting to purchase liquor (RCW 66.20.180).\n",
    "\n",
    "- When the subject is carrying a concealed pistol (RCW 9.41.050) failure to provide CPL is a civil infraction.\n",
    "\n",
    "Officers may not transport a person to any police facility or jail for the sole purpose of identifying them unless they have probable cause for arrest.\n",
    "\n",
    "While investigating a crime or possible crime, executing a search or arrest warrant, or issuing a citation or parks exclusion notice, officers may arrest subjects for false reporting SMC 12A.16.040(D) when subjects provide false written or oral identification.\n",
    "\n",
    "2. Officers Can Detain Subjects to Identify Them in Order to Issue a Notice of Infraction\n",
    "\n",
    "Under SMC 12A.02.140 and RCW 7.80.060, when an officer has probable cause to issue a Notice of Infraction for any City ordinance violation, the officer may detain the subject for a reasonable period of time to identify the subject.\n",
    "\n",
    "When officers have probable cause to issue a Notice of Infraction, and the subject refuses to identify themselves, the officer may request that a fingerprinting kit or Mobile ID be delivered to the scene and detain the subject for a reasonable amount of time to facilitate the fingerprinting.\n",
    "\n",
    "6.220 - POL – 4 Documenting a Terry Stop\n",
    "\n",
    "1. Officers Will Document All Terry Stops\n",
    "\n",
    "The documentation should contain all information requested in the Field Contact, but at a minimum will contain at least the following elements:\n",
    "\n",
    "- The original, objective facts justifying the reasonable suspicion for the stop or detention;\n",
    "\n",
    "- Any subsequent, objective facts that lengthen the original detention;\n",
    "\n",
    "- The scope and duration of the stop or detention;\n",
    "\n",
    "- The disposition of the stop or detention, including whether an arrest resulted;\n",
    "\n",
    "- Whether a frisk or consensual search was conducted;\n",
    "\n",
    "- The facts justifying the frisk or consensual search; and\n",
    "\n",
    "- The results of the frisk or consensual search\n",
    "\n",
    "- Demographic information pertaining to the subject, including perceived race, perceived age, and perceived gender; and\n",
    "\n",
    "- Any complications or delays that contributed to an inability to fill out all information on the Field Contact.\n",
    "\n",
    "Officers will clearly articulate the objective facts they rely upon in determining reasonable suspicion and probable cause.\n",
    "\n",
    "Officers will document all Terry stops on a Field Contact. Officers will use a separate Field Contact for each person seized during a Terry stop.\n",
    "\n",
    "- Officers are required to complete a Field Contact regardless of the outcome of the Terry stop.\n",
    "\n",
    "- Where an officer develops probable cause for arrest during the course of the stop, a Field Contact is still required.\n",
    "\n",
    "2. Officers Will Submit all Field Contacts Before They Leave at the End of their Shift\n",
    "\n",
    "Exception: Field Contacts documenting Off-Duty Terry Stops that do not lead to an arrest will be submitted by the completion of the next Department work shift.\n",
    "\n",
    "3. Officers Will Document All Other Detentions\n",
    "\n",
    "- Social contacts are not detentions and do not require documentation per this policy.\n",
    "\n",
    "- If the scope of the social contact evolves into a Terry stop, the officer will document the detention via a Field Contact.\n",
    "\n",
    "- Detentions based on probable cause do not require a Field Contact but require the officer to document the stop via a Report, Infraction/Citation, Traffic Contact Report, Trespass Warning, or Parks Trespass Warning/Exclusion.\n",
    "\n",
    "Supervisors will ensure the correct documentation of all detentions.\n",
    "\n",
    "4. Supervisors Will Review the Documentation of Terry Stops\n",
    "\n",
    "Absent extenuating circumstances, by the end of each shift, supervisors will review their officers’ Reports and Field Contacts that document the Terry stops made during the shift to determine if they were supported by reasonable suspicion and are consistent with SPD policy, federal, and state law.\n",
    "\n",
    "If a supervisor concludes that a Terry stop appears to be inconsistent with SPD policy, the supervisor, in consultation with their chain of command, shall address the concern and make the appropriate referral pursuant to Section 5.002. Such action may include PAS documentation and/or referral to OPA.  The supervisor shall document these concerns and any actions taken on a Supplement when approving the Report or Field Contact.\n",
    "\n",
    "- If a supervisor finds the documentation of the detention insufficient, the supervisor will return the documentation to the officer for corrections before the end of that shift.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
